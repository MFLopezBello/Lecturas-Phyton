Ejercitando con Apache Spark y Python a través de PySpark

El objetivo de esta guía es repasar los conceptos básicos de Spark a través de la resolución de ejercicios con la API para Python, PySpark, para dar soporte a la computación paralela sobre grandes colecciones de datos en un contexto de procesamiento distribuido.

image.png
Introducción a Spark

Las aplicaciones Spark se ejecutan como conjuntos independientes de procesos en un clúster, coordinados por el objeto SparkContext en su programa principal (llamado programa controlador).
Específicamente, para ejecutarse en un clúster, SparkContext puede conectarse a varios tipos de administradores de clústeres (ya sea el administrador de clúster independiente de Spark, Mesos o YARN), que asignan recursos entre aplicaciones. Una vez conectado, Spark adquiere ejecutores en nodos en el clúster, que son procesos que ejecutan cálculos y almacenan datos para su aplicación. A continuación, envía el código de su aplicación (definido por archivos JAR o Python pasados a SparkContext) a los ejecutores. Finalmente, SparkContext envía tareas a los ejecutores para que las ejecuten.
image.png
Arquitectura y Funcionamiento de Spark

En lineas generales, se plantea una serie de cuestiones respecto al funcionamiento de Spark en función del esquema anterior:
Cada aplicación tiene sus propios executor processes, que permanecen activos durante toda la aplicación y ejecutan tasks en varios subprocesos. Esto tiene la ventaja de aislar las aplicaciones entre sí, tanto en el lado de la programación (cada controlador programa sus propias tareas) como en el lado del ejecutor (las tareas de diferentes aplicaciones se ejecutan en diferentes JVM). Sin embargo, también significa que los datos no se pueden compartir entre diferentes aplicaciones Spark (instancias de SparkContext) sin escribirlos en un sistema de almacenamiento externo.
Spark es independiente del cluster manager subyacente. Siempre que pueda adquirir executor processes y estos se comuniquen entre sí, es relativamente fácil ejecutarlo incluso en un cluster manager que también admita otras aplicaciones (por ejemplo, Mesos o YARN).
El driver program debe escuchar y aceptar conexiones entrantes de sus ejecutores a lo largo de su vida. Como tal, el driver program debe ser direccionable en red desde los worker nodes.
Debido a que el driver programa tareas en el clúster, debe ejecutarse cerca de los worker nodes, preferiblemente en la misma red de área local.
Programando en modo PySpark

Ejercicio: Vamos a volver a trabajar sobre el ejercicio que resolvimos con MapReduce en el cual desarrollaremos un proceso que nos permita realizar un conteo de palabras para saber, en un supuesto dataset del orden de los petabytes, la cantidad de veces que aparece cada palabra.

Para comenzar a trabajar con PySpark, instalamos la librería pyspark, Java 8 y seteamos las variables de entorno para que no devuelva error:
 
[ ]
 !pip install pyspark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
Requirement already satisfied: pyspark in /usr/local/lib/python3.6/dist-packages (3.0.1)
Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from pyspark) (0.10.9)
Luego, inicializamos el Spark Context seteando el master y el nombre de la aplicación.
El siguiente ejemplos muestra la forma mínima de inicializar un SparkContext, donde se pasan dos parámetros:
Una URL de clúster, a saber, local en el ejemplo, que le dice a Spark cómo conectarse a un clúster. En Spark, local es un valor especial que ejecuta Spark en un thread en la máquina local, sin conectarse a un clúster.
Un nombre de aplicación, en nuestro caso Conteo de palabras. Este nombre identificará la aplicación en el cluster manager si nos conectamos a un clúster.
 
[ ]
 from pyspark import SparkConf, SparkContext

conf = SparkConf().setMaster("local").setAppName("Conteo de palabras")

# Inicializo el Spark Context
sc = SparkContext(conf = conf)

sc

Después de haber inicializado un SparkContext, ya podemos utilizar todos los métodos para crear y manipular RDDs, Dataframes, y Datasets que son las estructuras de datos presentes en Spark.
En primer lugar, tomamos el archivo de texto que vamos a procesar para hacer el conteo de palabras:
 
[ ]
 #!/usr/bin/env python
"""word_count.py"""

# Leemos el archivo
!wget https://raw.githubusercontent.com/bdm-unlu/2020/master/guias/utiles/mapreduce_spark/mr_text-file.txt

--2020-11-30 14:25:13--  https://raw.githubusercontent.com/bdm-unlu/2020/master/guias/utiles/mapreduce_spark/mr_text-file.txt
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 1420 (1.4K) [text/plain]
Saving to: ‘mr_text-file.txt.1’

mr_text-file.txt.1  100%[===================>]   1.39K  --.-KB/s    in 0s      

2020-11-30 14:25:13 (27.1 MB/s) - ‘mr_text-file.txt.1’ saved [1420/1420]

Trabajando con RDD's

Un RDD es simplemente una colección distribuida inmutable de elementos. En Spark, todo el trabajo se expresa ya sea creando nuevos RDD, transformando RDD existentes o llamando a operaciones en RDD para calcular un resultado. Bajo el capó, Spark distribuye automáticamente los datos contenidos en los RDD en su clúster y paraleliza las operaciones que realiza en ellos. Los RDD son el concepto central en Spark.

Si bien en las últimas versiones de Spark (mediados de 2019) el RDD como estructura de manipulación de datos pierde protagonismo, las estructuras nuevas que son utilizadas como los DataFrames y los DataSets son en realidad abstracciones de estos RDD que siguen estando en el núcleo de Spark.

Creación de RDD

Cada RDD se divide en varias particiones, que se pueden calcular en diferentes nodos del clúster. Los RDD pueden contener cualquier tipo de objetos Python, Java o Scala, incluidas las clases definidas por el usuario.
Es posbile crear RDD de dos maneras: cargando un conjunto de datos externo o distribuyendo una colección de objetos (por ejemplo, una lista o conjunto).
En este caso lo haremos a través de cargar un conjunto de datos externos con el método sc.textFile():
 
[ ]
 # Leo el archivo de texto
text_file = sc.textFile("mr_text-file.txt")
mr_text-file.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0
Una vez creados, los RDD ofrecen dos tipos de operaciones:
Las transformaciones construyen un nuevo RDD a partir de uno anterior.
Las acciones, por otro lado, calculan un resultado basado en un RDD y lo devuelven al programa del controlador o lo guardan en un sistema de almacenamiento externo (por ejemplo, HDFS).
Las transformaciones y las acciones son diferentes debido a la forma en que Spark calcula los RDD. Aunque puede definir nuevos RDD en cualquier momento, Spark solo los computa de forma diferida, es decir, la primera vez que se utilizan en una acción. Este enfoque puede parecer inusual al principio, pero tiene mucho sentido cuando se trabaja con Big Data. Si Spark cargara y almacenara todas las líneas en el archivo tan pronto como escribiéramos text_file = sc.textFile(...), desperdiciaría mucho espacio de almacenamiento o tiempo de procesamiento, dado que luego podemos filtrar muchas líneas. En cambio, una vez que Spark ve la cadena completa de transformaciones, puede calcular solo los datos necesarios para su resultado.
Acciones en Spark

A continuación se define un listado de las acciones más comunes y su utilización:
-Para la columna Result se calcula con un RDD con los valores {1, 2, 3, 3}.-
spark-acciones.png
Transformaciones en Spark

A continuación se define un listado de las transformaciones más comunes:
-Para la columna Result se calcula con un RDD con los valores {1, 2, 3, 3} en las operaciones unarias (map, flatMap, filter, distinct, sample) y para las binarias rdd={1, 2, 3} y other={3, 4, 5}.-
spark-transformaciones.png
Tip: ¿Cómo nos damos cuenta si ejecutamos una transformación o una acción? Si la operación devuelve un RDD es transformación, de lo contrario es acción.
Ahora que ocnocemos las RDD, las transformaciones y acciones, veamos como realizaríamos el wordCount con Spark:
 
[ ]
 # Separo en palabras
words = text_file.flatMap(lambda line: line.split(" "))

# Transformo a clave-valor
key_value = words.map(lambda word: (word, 1))

# Agrupo por clave
wordCounts = key_value.reduceByKey(lambda a,b:a +b)
    
# Muestro el primero
wordCounts.first()

# Muestro todos
for x in wordCounts.collect():
     print(x)

# Mostramos la cantidad
wordCounts.count()
('Aunque', 1)
('recién', 1)
('iniciará', 1)
('la', 12)
('octava', 1)
('fecha', 2)
('del', 5)
('torneo,', 1)
('el', 14)
('margen', 1)
('de', 12)
('error', 1)
('para', 2)
('Barcelona', 1)
('se', 2)
('reduce', 1)
('jornada', 2)
('a', 8)
('debido', 1)
('su', 4)
('irregular', 1)
('comienzo', 1)
('campeonato.', 1)
('En', 2)
('Estadio', 1)
('Mendizorroza,', 1)
('los', 1)
('catalanes', 1)
('buscan', 1)
('una', 1)
('victoria', 2)
('ante', 3)
('Alavés', 1)
('que', 5)
('les', 1)
('permita', 1)
('acomodarse', 1)
('dentro', 1)
('La', 1)
('Liga.', 1)
('Televisa', 1)
('ESPN', 1)
('2.', 1)
('', 3)
('Los', 1)
('catalanes,', 1)
('pese', 1)
('buen', 1)
('andar', 1)
('en', 6)
('plano', 1)
('internacional', 1)
('(goleada', 1)
('por', 2)
('5', 1)
('1', 1)
('Ferencvaros', 1)
('y', 6)
('2', 1)
('0', 1)
('Juventus', 1)
('Turín),', 1)
('certamen', 1)
('doméstico', 1)
('no', 2)
('encuentran', 1)
('rumbo,', 1)
('al', 4)
('cosechar', 1)
('solamente', 1)
('dos', 2)
('victorias,', 1)
('un', 1)
('empate', 1)
('derrotas', 1)
('(Getafe', 1)
('Real', 1)
('Madrid,', 1)
('Camp', 2)
('Nou).', 1)
('River', 2)
('presentó', 1)
('este', 1)
('sábado', 1)
('cerca', 1)
('las', 3)
('19.30', 1)
('respuesta', 1)
('oficial', 1)
('comunicado', 1)
('través', 1)
('cual', 1)
('Liga', 2)
('Profesional', 1)
('Fútbol', 1)
('rechazó', 1)
('como', 2)
('sede', 2)
('encuentro', 1)
('entre', 1)
('equipo', 1)
('Marcelo', 1)
('Gallardo', 1)
('Banfield.', 1)
('exposición,', 1)
('desde', 1)
('Núñez', 1)
('argumentan', 1)
('"resulta', 1)
('materialmente', 1)
('imposible"', 1)
('elegir', 1)
('otro', 1)
('estadio', 1)
('solicita', 1)
('sea', 1)
('AFA', 1)
('resuelva', 1)
('conflicto.', 1)
('"¡Ya', 1)
('tenemos', 1)
('parche', 2)
('campeón', 1)
('argentino!",', 1)
('celebró', 1)
('Boca', 1)
('redes', 1)
('sociales.', 1)
('Puede', 2)
('haber', 2)
('dado', 1)
('vuelta', 1)
('olímpica', 1)
('con', 1)
('copa', 2)
('campeón.', 2)
('ni', 1)
('siquiera', 1)
('recibido', 1)
('medallas.', 1)
('Después', 1)
('polémica', 1)
('generó', 1)
('previo', 1)
('última', 1)
('Superliga', 1)
('lejano', 1)
('mes', 1)
('marzo.', 1)
('Pero', 1)
('menos', 1)
('Xeneize', 1)
('ya', 1)
('tiene', 1)
('Copa', 1)
('frente', 1)
('Lanús', 1)
157
Finalmente, para apagar Spark, debemos llamaar al método stop () del SparkContext, en nuestro caso la variable sc:
 
[ ]
sc.stop()

Referencias

Learning Spark - Holden Karau, Andy Konwinski, Patrick Wendell and Matei Zaharia. O’Reilly Media, Inc. 2015. ISBN 978-1-449-35862-4
Spark Overview. http://spark.apache.org/docs/latest/
PySpark – Word Count Example. https://pythonexamples.org/pyspark-word-count-example/
A Neanderthal’s Guide to Apache Spark in Python https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427
